# Qwen2.5 Quantization Pipeline Configuration
# Pipeline: PiSSA -> SpinQuant -> GaLore

# Model Configuration
model_name: "Qwen/Qwen2.5-7B-Instruct"

# Output Configuration
output_dir: "outputs"
log_dir: "logs"

# Optional experiment trackers (GaLore training)
use_tensorboard: true
use_wandb: true
wandb:
  mode: "offline"  # offline|online|disabled
  project: "qwen-pipeline"
  entity: null

# Training Configuration
seed: 42
num_epochs: 3
batch_size: 4
gradient_accumulation_steps: 4
warmup_steps: 100
logging_steps: 10
save_steps: 500
save_total_limit: 3

# Dataset Configuration
# NOTE: This dataset is used for *adapter training* in the pipeline (PiSSA/SpinQuant/GaLore).
# It is NOT the same as the benchmarking datasets (AIME/MATH/GPQA/LiveCodeBench/SWE-Bench),
# which are configured separately in [`configs/benchmark_config.yaml`](configs/benchmark_config.yaml:1)
# and run via [`scripts/run_benchmarks.py`](scripts/run_benchmarks.py:1).
dataset: "c4"
dataset_split: "train"
max_samples: 10000
max_length: 512

# PiSSA Configuration
pissa:
  rank: 64
  alpha: 128
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"

# SpinQuant Configuration
spinquant:
  bits: 8  # Options: 4, 8
  double_quant: true
  quant_type: "nf4"  # Options: "nf4", "fp4"
  group_size: 128
  symmetric: false
  backend: "blockwise_givens"
  block_size: 64
  num_steps: 50
  lr: 0.05
  num_sweeps: 2
  max_layers: 16
  use_bnb_quantization: true
  use_activation_objective: true
  calibration_vectors_per_layer: 512
  keep_fp16_modules:
    - "input_embeddings"
    - "output_embeddings"

# GaLore Configuration
galore:
  rank: 128
  learning_rate: 1.0e-4
  weight_decay: 0.01
  update_proj_gap: 200
  scale: 0.25
  proj_type: "std"  # Options: "std", "reverse_std", "full"

# Optional: post-quant adapter distillation
distill:
  enabled: false
  alpha: 0.5
  temperature: 2.0

# GaLore-like gradient projection (projection-space updates)
projection:
  enabled: true
  rank: 64
  update_gap: 200
  drift_threshold: 0.35

  # Optional schedules (step-based). If provided, these override `rank`/`update_gap`
  # as training progresses.
  # Example:
  # rank_schedule:
  #   - {step: 0, value: 32}
  #   - {step: 200, value: 64}
  # update_gap_schedule:
  #   - {step: 0, value: 400}
  #   - {step: 1000, value: 200}

  # Optional scheduler (alternative to step lists): cosine decay for update_gap.
  # This yields infrequent projection updates early (large gap), more frequent
  # updates mid-run, and the most frequent updates late (small gap).
  #
  # Enable by setting:
  # update_gap_schedule_mode: "cosine"
  # update_gap_cosine:
  #   max_gap: 400
  #   min_gap: 50
  #
  # Notes:
  # - If `update_gap_schedule` is set, it takes precedence over cosine.
