@misc{qwen2024qwen25,
  title        = {Qwen2.5 Technical Report},
  author       = {Qwen Team},
  year         = {2024},
  howpublished = {arXiv preprint},
  note         = {arXiv:2409.*** (placeholder)}
}

@inproceedings{hu2022lora,
  title     = {LoRA: Low-Rank Adaptation of Large Language Models},
  author    = {Hu, Edward and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2022}
}

@inproceedings{dettmers2023qlora,
  title     = {QLoRA: Efficient Finetuning of Quantized LLMs},
  author    = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2023}
}

@misc{pissa2024,
  title        = {PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models},
  author       = {Anonymous},
  year         = {2024},
  howpublished = {arXiv preprint},
  note         = {arXiv:2404.*** (placeholder)}
}

@misc{spinquant2024,
  title        = {SpinQuant: Rotation-Aware Post-Training Quantization for Large Language Models},
  author       = {Anonymous},
  year         = {2024},
  howpublished = {arXiv preprint},
  note         = {arXiv:2405.*** (placeholder)}
}

@misc{galore2024,
  title        = {GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection},
  author       = {Anonymous},
  year         = {2024},
  howpublished = {arXiv preprint},
  note         = {arXiv:2403.*** (placeholder)}
}

@inproceedings{frantar2023gptq,
  title     = {GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author    = {Frantar, Elias and Alistarh, Dan},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2023}
}

@misc{lin2023awq,
  title        = {AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author       = {Lin, Ji and Tang, Jiaming and Tang, Haotian and others},
  year         = {2023},
  howpublished = {arXiv preprint},
  note         = {arXiv:2306.*** (placeholder)}
}

@misc{xiao2023smoothquant,
  title        = {SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author       = {Xiao, Guangxuan and others},
  year         = {2023},
  howpublished = {arXiv preprint},
  note         = {arXiv:2211.*** (placeholder)}
}

@misc{dettmers2022bitsandbytes,
  title        = {bitsandbytes: 8-bit Optimizers and Quantization Routines},
  author       = {Dettmers, Tim and others},
  year         = {2022},
  howpublished = {GitHub repository},
  note         = {\url{https://github.com/TimDettmers/bitsandbytes}}
}

