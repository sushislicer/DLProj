\begin{abstract}
Large language models (LLMs) are increasingly deployed under tight memory and latency budgets, yet many applications still require task adaptation.
Existing approaches often optimize only one axis: low-rank adapters reduce trainable parameters, while post-training quantization compresses weights for inference.
This paper proposes \method, a practical pipeline that couples (i) \,PiSSA-initialized low-rank adapters, (ii) rotation-aware residual quantization inspired by SpinQuant, and (iii) GaLore-style gradient projection during adapter training.
At a high level, we view adaptation as learning a low-rank update while simultaneously controlling quantization noise in the base (or residual) weights through orthogonal mixing.
To keep runtime bounded on large models, our SpinQuant-lite backend defaults to a fixed blockwise Hadamard rotation, avoiding expensive manifold optimization while still reducing quantization error.

We evaluate on difficult reasoning and knowledge benchmarks implemented in our benchmarking suite (AIME, MATH, GPQA, and LiveCodeBench) across \qwen model sizes.
Results will be populated from the provided results directory once benchmarking completes.
\end{abstract}
