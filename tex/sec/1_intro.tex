\section{Introduction}
\label{sec:intro}

Quantization and parameter-efficient fine-tuning are now standard tools for deploying large language models (LLMs) under constrained memory.
However, practitioners still face a persistent mismatch between \emph{how} models are trained and \emph{how} they are served.
On the training side, low-rank adaptation methods such as LoRA and its variants reduce optimizer state and training memory~\cite{hu2022lora,dettmers2023qlora}.
On the inference side, 4-bit quantization drastically compresses weights, but can change layerwise scale and introduce structured errors that degrade reasoning benchmarks.

This paper targets a practical setting: adapting \qwen models with limited compute, while maintaining a quantized deployment footprint.
We propose \method, a simple pipeline that composes three complementary ideas:
\begin{itemize}
  \item \textbf{PiSSA-initialized adapters.} We use PiSSA initialization to obtain low-rank adapter parameters aligned with the dominant singular subspace of each target weight matrix.
  This improves optimization stability relative to random initialization when only a small number of update steps are affordable.
  \item \textbf{SpinQuant-inspired rotation-aware quantization.} We apply an orthogonal mixing transform before quantization of the residual/base weights.
  In our implementation, we default to a fixed blockwise Hadamard rotation to avoid expensive per-layer manifold optimization while still reducing quantization error.
  \item \textbf{GaLore-style gradient projection.} During adapter training, we optionally project 2D gradients onto a low-rank subspace, improving memory and compute efficiency of updates.
\end{itemize}

\paragraph{Contributions.} This work makes the following contributions:
\begin{itemize}
  \item A unified training-to-deployment pipeline for low-rank adaptation and rotation-aware quantization (\method).
  \item A practical SpinQuant-lite backend with a fast fixed-rotation path suitable for multi-GPU benchmark sweeps.
  \item A concise mathematical view that connects low-rank adaptation, orthogonal mixing for quantization, and projected-gradient training.
  \item An evaluation protocol and benchmark harness for difficult reasoning tasks (AIME, MATH, GPQA, LiveCodeBench).
\end{itemize}
