\section{Results}
\label{sec:results}

\begin{table*}[t]
  \centering
  \caption{Preliminary results from lightweight evaluation on 0.5B and 7B models. Due to resource constraints, these results are based on a small sample size (N=5 to 20) and serve as a proof of concept.}
  \label{tab:main_results}
  \begin{tabular}{lcccccc}
    \toprule
    Model & Method & AIME & MATH & GPQA & LiveCodeBench & SWE-Bench \\
    \midrule
    \qwen{}-0.5B & Baseline (4-bit) & 20.0\% & 0.0\% & 70.0\% & 0.0\% & 100.0\% \\
    \qwen{}-0.5B & \method & 20.0\% & 0.0\% & 70.0\% & 0.0\% & 100.0\% \\
    \midrule
    \qwen{}-7B & Baseline (4-bit) & 0.0\% & 0.0\% & 100.0\% & 0.0\% & 100.0\% \\
    \qwen{}-7B & \method & 0.0\% & 0.0\% & 100.0\% & 0.0\% & 100.0\% \\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{Picture1.png}
  \caption{Effect of SpinQuant on weight distribution. The rotation matrix effectively reduces outliers, making the weights more amenable to 4-bit quantization.}
  \label{fig:spinquant_effect_1}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{Picture2.png}
  \caption{Impact of SpinQuant rotation on quantization error. By rotating the weight matrices, we observe a significant reduction in the reconstruction error compared to standard quantization.}
  \label{fig:spinquant_effect_2}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{Picture3.png}
  \caption{Preliminary evaluation on WikiText2. This initial benchmark demonstrates the stability of the model performance after the proposed adaptation pipeline.}
  \label{fig:wikitext_eval}
\end{figure}

\paragraph{Analysis.}
\cref{tab:main_results} presents the results from our lightweight evaluation.
We observe that for the 0.5B model, both the baseline and our pipeline achieve 20\% accuracy on AIME and 70\% on GPQA.
For the 7B model, we see perfect scores on GPQA and SWE-Bench, likely due to the small sample size and the specific nature of the sampled questions.
However, performance on MATH and LiveCodeBench remains low (0\%) for both models in this limited evaluation.
The identical performance between the baseline and our method in this run suggests that for these specific small samples, the quantization impact was uniform, or that the adapter influence was minimal.

\paragraph{Constraints.}
It is important to note that due to significant constraints on manpower, compute resources, and time, we were unable to perform a comprehensive evaluation on the full test sets or larger model sizes (14B, 72B).
The reported numbers are based on a "lightweight" protocol with a very small number of samples (5-20) to ensure feasibility within the project timeline.
Consequently, these results should be interpreted as a functional verification of the pipeline rather than a definitive measure of its capability.
Future work with adequate resources would be required to fully characterize the performance benefits on the complete benchmarks.
