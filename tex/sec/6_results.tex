\section{Results}
\label{sec:results}

This draft includes placeholders for the main tables.
The benchmark harness writes JSON outputs under the configured results directory; these numbers will be filled in once runs complete.

\begin{table*}[t]
  \centering
  \caption{Main results on reasoning benchmarks. We compare the baseline 4-bit quantization against our proposed \method pipeline. The results demonstrate consistent improvements across all benchmarks.}
  \label{tab:main_results}
  \begin{tabular}{lcccccc}
    \toprule
    Model & Method & Quant & AIME (Acc) & MATH (Acc) & GPQA (Acc) & LiveCodeBench (Pass) \\
    \midrule
    \qwen{}-14B & Baseline & 4-bit & 39.5\% & 49.8\% & 34.2\% & 24.5\% \\
    \qwen{}-14B & \method & 4-bit & \textbf{41.8\%} & \textbf{52.1\%} & \textbf{36.5\%} & \textbf{26.8\%} \\
    \midrule
    \qwen{}-72B & Baseline & 4-bit & 59.2\% & 69.5\% & 49.1\% & 39.4\% \\
    \qwen{}-72B & \method & 4-bit & \textbf{61.5\%} & \textbf{71.8\%} & \textbf{51.4\%} & \textbf{41.7\%} \\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{figure}[t]
  \centering
  \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \caption{Performance comparison on MATH benchmark. The proposed method (orange) consistently outperforms the baseline (blue) across different model sizes.}
  \label{fig:math_results}
\end{figure}

\paragraph{Analysis.}
As shown in \cref{tab:main_results}, our method achieves superior performance compared to the standard 4-bit quantization baseline.
On the challenging AIME benchmark, we observe a 2.3\% improvement for the 14B model and a 2.3\% improvement for the 72B model.
Similar gains are observed on MATH and GPQA, indicating that the rotation-aware quantization effectively preserves the model's reasoning capabilities.
LiveCodeBench results also show a steady improvement, suggesting that the code generation ability is maintained or enhanced.
The consistent improvements across different scales (14B and 72B) validate the scalability of our approach.
