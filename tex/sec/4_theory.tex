\section{Theory: Tying Low-Rank Adaptation and Rotation-Aware Quantization}
\label{sec:theory}

This section gives a compact mathematical view of why \method combines naturally.
The goal is not to provide a tight new bound, but to make explicit the shared structure of the three components.

\subsection{A constrained view of adaptation under quantized deployment}
Let $\theta$ denote base weights and $\phi$ denote adapter parameters.
We can view the deployment-constrained objective as
\begin{equation}
  \begin{aligned}
    \min_{\phi,\,\{\R_\ell\}}\;& \E\,\ell\Big(f\big(x;\{\Q_b(\W_\ell\R_\ell),\phi\}\big),y\Big)\\
    \text{s.t.}\;& \R_\ell^\top\R_\ell=\I,\qquad \mathrm{rank}(\Delta\W_\ell)\le r.
  \end{aligned}
  \label{eq:joint_obj}
\end{equation}
Here $\ell$ indexes linear layers, $\Delta\W_\ell$ is the low-rank update induced by adapters, and $\Q_b$ is a low-bit quantizer.
\method can be interpreted as an alternating approximation to \cref{eq:joint_obj}:
PiSSA provides an initialization for the low-rank constraint, SpinQuant-lite fixes a structured choice of $\R_\ell$, and GaLore accelerates optimization over $\phi$.

\subsection{A simple error decomposition}
Consider a single linear layer with quantized residual weights and a low-rank adapter:
\begin{equation}
  \W_{\text{serve}} \;=\; \Q_b(\W_{\mathrm{res}}\R)\,\R^\top + \Delta\W.
  \label{eq:serve_weight}
\end{equation}
For an input activation $\bm{x}$, the output error relative to a full-precision adapted layer $\W_{\mathrm{res}}+\Delta\W$ is
\begin{equation}
  \delta\bm{y}
  = \bm{x}\Big(\Q_b(\W_{\mathrm{res}}\R)\,\R^\top - \W_{\mathrm{res}}\Big)^\top.
  \label{eq:output_error_residual}
\end{equation}
This highlights the role of rotations: $\R$ changes the distribution of entries in $\W_{\mathrm{res}}\R$ and thus the quantization perturbation, without changing $\|\W_{\mathrm{res}}\|_F$.
In practice, a fixed Hadamard transform is an inexpensive way to reduce structured quantization error.

\subsection{Why orthogonal mixing helps quantization}
Quantization introduces an error $\bm{E}_\ell = \Q_b(\W_\ell\R_\ell) - \W_\ell\R_\ell$.
For an input activation $\bm{x}$, the induced output perturbation is
\begin{equation}
  \delta \bm{y} \;=\; (\bm{x}\R_\ell^\top)\,\bm{E}_\ell^\top.
  \label{eq:delta_y}
\end{equation}
For many quantizers, the magnitude of $\bm{E}_\ell$ depends on the distribution of entries (or per-channel scales) in $\W_\ell\R_\ell$.
Orthogonal mixing redistributes energy across dimensions without changing the Frobenius norm:
\begin{equation}
  \|\W_\ell\R_\ell\|_F = \|\W_\ell\|_F.
\end{equation}
In particular, a Hadamard transform approximately ``whitens'' coordinates by mixing signs and permuting mass, which empirically reduces outlier dominance and improves quantization fidelity.
This provides an inexpensive alternative to learned rotations when runtime is limited.

\paragraph{A simple bound (informal).}
Assume the layer input activations satisfy $\|\bm{x}\|_2 \le B$.
Then from \cref{eq:delta_y},
\begin{equation}
  \|\delta\bm{y}\|_2 \le B\,\|\bm{E}_\ell\|_2 \le B\,\|\bm{E}_\ell\|_F.
  \label{eq:error_bound}
\end{equation}
Thus any rotation $\R_\ell$ that reduces the typical quantization residual norm $\|\bm{E}_\ell\|_F$ (e.g., by suppressing outlier concentration) improves a worst-case output perturbation bound.

\subsection{Projected gradients as implicit low-rank regularization}
GaLore-style projection in \cref{eq:galore_proj} can be interpreted as optimizing with gradients constrained to a rank-$k$ tangent set.
For a 2D parameter $\bm{P}$, the update $\bm{P}\leftarrow \bm{P}-\eta\bm{G}_{\text{proj}}$ is the steepest descent direction under the restriction that the update lies in the span defined by $(\U,\V)$.
When applied to adapter parameters (already low-dimensional), this can stabilize updates and reduce the effective optimizer memory footprint.

\subsection{Putting it together}
Under quantized deployment, training should account for quantization-induced perturbations.
\method operationalizes this principle: PiSSA gives a strong low-rank starting point, SpinQuant-lite reduces quantization error of the (residual) base weights by orthogonal mixing, and GaLore accelerates adapter optimization under tight budgets.
